---
layout: default
title: "BNP for ML"
description: "Material for a course at ENSAE Paris, 2017"
---

<section>
        <h1> An introduction to Bayesian nonparametrics</h1>
<article>
	<p>
      Here are <a href="../material/BNP/slidesBNP.pdf"> slides </a> for the course given at ENSAE Paris in January 2017. The IPython notebooks I used for demos are:
      <ul>
          <li>
              <a href="../material/BNP/gaussianProcesses.ipynb"> here </a> for an introduction to GPs and Bayesian optimization.
          </li>
          <li>
              <a href="../material/BNP/gaussianProcesses.ipynb"> here </a> for the CO2 example from Rasmussen and Williams's book. The notebook comes from the documentation of <a href="http://scikit-learn.org/stable/modules/gaussian_process.html">scikit-learn</a>.
          </li>
      </ul>
   </p>

        <p>
            Here are some of the resources I used on Gaussian processes
            <ul>
                <li> the 2006 <a href="http://www.gaussianprocess.org/gpml/chapters"> GPML textbook</a> by Rasmussen and Williams is a great overview of GPs for ML, with a focus on methods.
                </li>
                <li>
                    a <a href="http://videolectures.net/mlss09uk_rasmussen_gp/?q=rasmussen"> videolecture </a> by Rasmussen that skims through the book.
                </li>
                <li>
                    The 2014 <a href="http://ce.sharif.edu/courses/93-94/2/ce957-1/resources/root/References/porbanz_BNP.pdf"> BNP lecture notes</a> by Orbanz are excellent if you want a gentle and mathematically clean entry to the theory of BNP. For GPs, you'll find a discussion on existence, conditioning, Bayes' theorem in undominated models.  
                </li>
            </ul>
            </p>

            <p>
                On Dirichlet processes and the Chinese restaurant process, you can refer to
                <ul>
                    <li>
                        Orbanz' 2014 <a href="http://ce.sharif.edu/courses/93-94/2/ce957-1/resources/root/References/porbanz_BNP.pdf"> BNP lecture notes</a> again for a nice overview of relevant theory.
                    </li>
                    <li>
                        Chapters 1, 2, 5, 6 of the 2010 <a href="https://www.amazon.com/Nonparametrics-Cambridge-Statistical-Probabilistic-Mathematics/dp/0521513464"> book</a> edited by Hjort, Holmes et al. This book isa collection of dense independent chapters and not really a textbook, more of a state-of-the-art with a lot of pointers. There is ample material for those who want to read further on BNP.
                    </li>
                    <li> There is an upcoming book by YW Teh and M Jordan that should become the reference ML textbook that [Rasmussen and Williams, 2006] is to GPs, keep an eye open! Meanwhile, recent videolectures by <a href="http://videolectures.net/mlss2011_teh_nonparametrics/"> Teh</a> and <a href="http://videolectures.net/icml05_jordan_dpcrp/?q=jordan%20nonparametrics"> M Jordan</a> are probably a good approximation to the book's content.
                    </li>
                </ul>
</article>
</section>
